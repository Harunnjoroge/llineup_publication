{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62194658",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# This notebook generates LD for pre intervention and post intervention population. Ne is estimated from the calculated LD. \n",
    "# The two metrics assess change in population size during the llineup trial. \n",
    "\n",
    "#################################################################################################################################################################\n",
    "\n",
    "# Load packages\n",
    "\n",
    "import pickle\n",
    "import allel\n",
    "import numpy as np\n",
    "import itertools\n",
    "import random\n",
    "from scipy import stats\n",
    "import os\n",
    "import malariagen_data\n",
    "from re import *\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ensure reproducibility \n",
    "random.seed(42)\n",
    "\n",
    "# Write a function to calculate burrows' delta from a pair of loci\n",
    "def burrows_delta(calls, allele_i, allele_j, unbiased = True):\n",
    "\tallele_num = calls.shape[1] * 2\n",
    "\t# This only works if the alleles can only be 0 or 1\n",
    "\tp = ((np.sum(calls[0]) * allele_i) + (allele_num - np.sum(calls[0])) * (1 - allele_i)) / allele_num\n",
    "\tq = ((np.sum(calls[1]) * allele_j) + (allele_num - np.sum(calls[1])) * (1 - allele_j)) / allele_num\n",
    "\thom_i = 2*allele_i\n",
    "\thom_j = 2*allele_j\n",
    "\thet = 1\n",
    "\tdouble_homozygotes = np.sum(np.logical_and(calls[0] == hom_i, calls[1] == hom_j))\n",
    "\thom_i__het_j = np.sum(np.logical_and(calls[0] == hom_i, calls[1] == het))\n",
    "\thet_i__hom_j = np.sum(np.logical_and(calls[0] == het, calls[1] == hom_j))\n",
    "\thet__het = np.sum(np.logical_and(calls[0] == het, calls[1] == het))\n",
    "\tdelta = (2*double_homozygotes +\n",
    "\t         hom_i__het_j +\n",
    "\t         het_i__hom_j +\n",
    "\t         het__het / 2\n",
    "\t        ) / calls.shape[1] - 2*p*q\n",
    "\tif unbiased:\n",
    "\t\tdelta = delta*calls.shape[1] / (calls.shape[1] - 1)\n",
    "\treturn(delta)\n",
    "\n",
    "# Write a function to calculate r_hat_delta:\n",
    "def Weir_r_hat_delta(calls, allele_i, allele_j):\n",
    "\tallele_num = calls.shape[1] * 2\n",
    "\t# This only works if the alleles can only be 0 or 1\n",
    "\tp = ((np.sum(calls[0]) * allele_i) + (allele_num - np.sum(calls[0])) * (1 - allele_i)) / allele_num\n",
    "\tq = ((np.sum(calls[1]) * allele_j) + (allele_num - np.sum(calls[1])) * (1 - allele_j)) / allele_num\n",
    "\thom_i = 2*allele_i\n",
    "\thom_j = 2*allele_j\n",
    "\th_i = np.sum(calls[0] == hom_i) / calls.shape[1]\n",
    "\th_j = np.sum(calls[1] == hom_j) / calls.shape[1]\n",
    "\tdelta_hat = burrows_delta(calls, allele_i, allele_j)\n",
    "\tr_hat_delta = delta_hat / np.sqrt( (p*(1-p) + (h_i - p**2)) * (q*(1-q) + (h_j - q**2)) )\n",
    "\treturn(r_hat_delta)\n",
    "\n",
    "# For a given pair of loci, calculate the mean of the r values for all pairs of alleles\n",
    "def mean_r_squared_delta(calls):\n",
    "\t# Remove samples with missing calls.\n",
    "\tmissing_calls = np.any(calls < 0, 0)\n",
    "\tcalls = calls[:, ~missing_calls]\n",
    "\tr_00 = Weir_r_hat_delta(calls, 0, 0) ** 2\n",
    "\tr_01 = Weir_r_hat_delta(calls, 0, 1) ** 2\n",
    "\tr_10 = Weir_r_hat_delta(calls, 1, 0) ** 2\n",
    "\tr_11 = Weir_r_hat_delta(calls, 1, 1) ** 2\n",
    "\treturn(np.mean([r_00, r_01, r_10, r_11]))\n",
    "\n",
    "# Calculate the expected r^2 due only to sampling (see Table 1 in Weir 1979), based on sample size S\n",
    "def expected_r_squared(S):\n",
    "\tif S < 30:\n",
    "\t\tE = 0.0018 + 0.907/S + 4.44/(S**2)\n",
    "\telse:\n",
    "\t\tE = 1/S + 3.19/(S**2)\n",
    "\treturn(E)\n",
    "\n",
    "# For a set of genotype calls, calculate the mean r_squared, adjusted to the expectation. In case there are\n",
    "# more locus permutations than are computationally feasible, we can set the number of allele pairs to randomly\n",
    "# choose (N_subset). If N_subset = None, use all permutations\n",
    "def overall_r_squared(call_set, N_subset = 100000, mindist = 1, positions = None):\n",
    "\t# If we want the minimum distance between markers to be greater than 1, then we need to know the positions of the\n",
    "\t# markers\n",
    "\tif mindist > 1 and positions is None:\n",
    "\t\traise Exception('If mindist > 1, a list of marker positions is needed.')\n",
    "\t# What is the number of possible permutations?\n",
    "\tpossible_permutations = int(call_set.shape[0] * (call_set.shape[0] - 1) / 2)\n",
    "\t# If the number of possible permutations is smaller than N_subset, or if N_subset is None, use all permutations\n",
    "\tif N_subset is None or N_subset >= possible_permutations:\n",
    "\t\t# Create an iterator for the permutations\n",
    "\t\tpermutations = itertools.combinations(range(call_set.shape[0]), r = 2)\n",
    "\t\t# Set up the vector of r values\n",
    "\t\tr = np.array([])\n",
    "\t\tfor p in permutations:\n",
    "\t\t\tif mindist > 1:\n",
    "\t\t\t\tthis_distance = positions[p[1]] - positions[p[0]]\n",
    "\t\t\t\tif this_distance < mindist:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\tthese_calls = call_set[p, :]\n",
    "\t\t\tr = np.append(r, mean_r_squared_delta(these_calls))\n",
    "\t# Otherwise, use a random subset of permutations\n",
    "\telse :\n",
    "\t\t# the code below calculated all permutations and then chose a random subset of them. I think that for the\n",
    "\t\t# number of locus pairs that we are thinking of using, it will be a lot quicker to just sample them randomly\n",
    "\t\t# and kick out any repeats\n",
    "#\t\t# Create an iterator for the permutations\n",
    "#\t\tpermutations = itertools.combinations(range(call_set.shape[0]), r = 2)\n",
    "#\t\t# Choose which permutations we will use\n",
    "#\t\tselect_permutations = np.sort(random.sample(range(possible_permutations), N_subset))\n",
    "#\t\t# The islice function will skip a number of steps ahead in the iterator, so we need to change this array of\n",
    "#\t\t# choices into the number of steps to skip from the previous one\n",
    "#\t\tselect_steps = select_permutations - np.concatenate([[0], select_permutations[:-1] + 1])\n",
    "#\t\t# Set up the vector of r values\n",
    "#\t\tr = np.array([])\n",
    "#\t\tfor i, s in enumerate(select_steps):\n",
    "#\t\t\tp = next(itertools.islice(permutations, s, s+1))\n",
    "#\t\t\tif mindist > 1:\n",
    "#\t\t\t\tthis_distance = positions[p[1] - positions[p[0]]]\n",
    "#\t\t\t\tif this_distance < mindist:\n",
    "#\t\t\t\t\tcontinue\n",
    "#\t\t\tthese_calls = call_set[p, :]\n",
    "#\t\t\tr = np.append(r, mean_r_squared_delta(these_calls))\n",
    "\t\t# We choose pairs of loci at random. I'm going to be lazy here and make it so we never use the same locus\n",
    "\t\t# twice. This will cause problems if N_subset is ever larger than the call_set / 2, but I don't think that\n",
    "\t\t# this is realistically going to happen.\n",
    "\t\tgamete1 = np.array(random.sample(range(call_set.shape[0]), N_subset))\n",
    "\t\tunused = np.setdiff1d(np.array(range(call_set.shape[0])), gamete1)\n",
    "\t\tgamete2 = np.random.choice(unused, N_subset, replace = False)\n",
    "\t\tpermutations = zip(gamete1, gamete2)\n",
    "\t\tr = np.array([])\n",
    "\t\tfor p in permutations:\n",
    "\t\t\tif mindist > 1:\n",
    "\t\t\t\tthis_distance = positions[p[1] - positions[p[0]]]\n",
    "\t\t\t\tif this_distance < mindist:\n",
    "\t\t\t\t\tcontinue\n",
    "\t\t\tthese_calls = call_set[p, :]\n",
    "\t\t\tr = np.append(r, mean_r_squared_delta(these_calls))\n",
    "\treturn(r)\n",
    "\n",
    "# Write a function to calculate r_squared using variants from two different chromosomes.\n",
    "# N_subset is a bit misleading here. If you set it to None, the function will take all pairwise combinations of SNPs\n",
    "# between the two chromosomes. But if you set N_subset to a value, it will just take that many pairs. So setting\n",
    "# N_subset to the number of SNPs is not the same as setting it to None.\n",
    "def twochrom_r_squared(call_set1, call_set2, N_subset=None):\n",
    "\tif N_subset is None or N_subset > min([call_set1.shape[0], call_set2.shape[0]]):\n",
    "\t\t# Create an iterator for the permutations\n",
    "\t\tpermutations = itertools.product(range(call_set1.shape[0]), range(call_set2.shape[0]))\n",
    "\t\tprint('Using all pairwise combinations.')\n",
    "\telse:\n",
    "\t\tgamete1 = np.array(random.sample(range(call_set1.shape[0]), N_subset))\n",
    "\t\tgamete2 = np.array(random.sample(range(call_set2.shape[0]), N_subset))\n",
    "\t\tpermutations = zip(gamete1, gamete2)\n",
    "\t\tprint('Using a subset of loci with one pair per selected locus.')\n",
    "\t# Set up the vector of r values\n",
    "#\tr = np.array([mean_r_squared_delta(np.vstack([call_set1[p[0], :], call_set2[p[1], :]])) for p in permutations])\n",
    "\tr = np.array([])\n",
    "\tfor p in permutations:\n",
    "\t\tthese_calls = np.vstack([call_set1[p[0], :], call_set2[p[1], :]])\n",
    "\t\tr = np.append(r, mean_r_squared_delta(these_calls))\n",
    "\treturn(r)\n",
    "\n",
    "# Write a function to calculate effective population size from the r_squared values and sample size\n",
    "def Weir_Ne(r_squared_vector, S, c = None):\n",
    "\texpected = expected_r_squared(S)\n",
    "\t\n",
    "\tobserved = np.mean(r_squared_vector)\n",
    "\tadjusted = observed - expected\n",
    "\t# If no recombination rate was given, use the equations from Waples & Do 2008\n",
    "\tif c is None:\n",
    "\t\tif S < 30:\n",
    "\t\t\tNe = (0.308 + np.sqrt(0.308**2 - 2.08 * adjusted)) / (2*adjusted)\n",
    "\t\telse:\n",
    "\t\t\tNe = ((1/3) + np.sqrt((1/9) - 2.76 * adjusted)) / (2*adjusted)\n",
    "\t# Otherwise, use the equations from Appendix 1 of Hollenbeck et al 2016\n",
    "\telse:\n",
    "\t\tgamma = ((1-c)**2 + c**2)/(2*c*(2-c))\n",
    "\t\tNe = gamma / adjusted\n",
    "\tif Ne < 0:\n",
    "\t\treturn np.Inf\n",
    "\telse:\n",
    "\t\treturn(Ne)\n",
    "\n",
    "# Write a function that will take genotypes and calculate the r squared between the two chromosomes\n",
    "def process_genotypes(genotypes_1, genotypes_2, sample_size = None, all_pairwise_thresh = 100000):\n",
    "\tif (genotypes_1.shape[1] != genotypes_2.shape[1]):\n",
    "\t\traise Exception('The two genotypes should have the same number of samples. ')\n",
    "\tif sample_size is None:\n",
    "\t\tg_full_1 = genotypes_1[:,:]\n",
    "\t\tg_full_2 = genotypes_2[:,:]\n",
    "\telse:\n",
    "\t\tg_full_1 = genotypes_1[:, :sample_size]\n",
    "\t\tg_full_2 = genotypes_2[:, :sample_size]\n",
    "\t# remove singletons\n",
    "\tsingleton_filter_1 = np.logical_and(np.sum(g_full_1, 1) > 1, np.sum(g_full_1, 1) < (2*g_full_1.shape[1] - 1))\n",
    "\tg_full_1 = g_full_1[singleton_filter_1, :]\n",
    "\tsingleton_filter_2 = np.logical_and(np.sum(g_full_2, 1) > 1, np.sum(g_full_2, 1) < (2*g_full_2.shape[1] - 1))\n",
    "\tg_full_2 = g_full_2[singleton_filter_2, :]\n",
    "\t# Remove sites where every sample is heterozygous, since this throws off the calculations,\n",
    "\t# and also these are probably dodgy sites anyway.\n",
    "\tfull_het_filter_1 = np.all(g_full_1 == 1, 1)\n",
    "\tg1 = g_full_1[~full_het_filter_1, :]\n",
    "\tfull_het_filter_2 = np.all(g_full_2 == 1, 1)\n",
    "\tg2 = g_full_2[~full_het_filter_2, :]\n",
    "\t# now calculate rsquard\n",
    "\tnum_loci = min([g1.shape[0], g2.shape[0]])\n",
    "\tif num_loci < all_pairwise_thresh:\n",
    "\t\trs = twochrom_r_squared(g1, g2)\n",
    "\telse:\n",
    "\t\trs = twochrom_r_squared(g1, g2, num_loci)\n",
    "\treturn(g1, g2, rs)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4225287b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(null);\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-3.2.2.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-3.2.2.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    }
   ],
   "source": [
    "\n",
    "#load malariagen data\n",
    "ag3 = malariagen_data.Ag3(pre = True)\n",
    "sample_sets=[\"1288-VO-UG-DONNELLY-VMF00168\",\"1288-VO-UG-DONNELLY-VMF00219\"]\n",
    "meta = ag3.sample_metadata(\n",
    "    sample_sets=sample_sets, \n",
    "    sample_query = \"aim_species == 'gambiae'\"\n",
    ")\n",
    "# Remove decimal and numbers after it in the \"partner_sample_id\" column not present in our meta data\n",
    "meta['partner_sample_id'] = meta['partner_sample_id'].str.split('.').str[0]\n",
    "\n",
    "# Convert \"partner_sample_id\" column in meta to float64 to match llineup_meta dtype\n",
    "meta['partner_sample_id'] = pd.to_numeric(meta['partner_sample_id'])\n",
    "\n",
    "#load llineup trial metadata\n",
    "llineup_meta = pd.read_csv('~/lstm_scratch/network_scratch/llineup/llineup-genomics/data/ento_geno_plasmo_data.csv',\n",
    "                       index_col = 0,\n",
    "                      ).query(\"species=='An. gambiae'\")\n",
    "llineup_meta= llineup_meta.drop_duplicates(subset=['wgs.sample.id'])\n",
    "llineup_meta= llineup_meta.set_index('wgs.sample.id')\n",
    "llineup_meta = llineup_meta.reindex(index=meta['partner_sample_id'])#match order in malariagen meta data\n",
    "llineup_meta=llineup_meta.reset_index()\n",
    "llineup_meta['sample_id'] =meta['sample_id']#create sample id to merge with ag3 metadata\n",
    "llineup_meta['LLIN_actual']=llineup_meta['LLIN.actual']\n",
    "\n",
    "#add column control phase for grouping samples to pre and post interventions\n",
    "rnd_map = {1: 'pre', 5: 'post'}\n",
    "\n",
    "llineup_meta['control_phase'] = llineup_meta['RND'].map(rnd_map).fillna('intermediate')\n",
    "ag3.add_extra_metadata(llineup_meta)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91475529",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get genotype calls and exclude the 2Rb and 2La chromosomal inversions in  chromosome 2\n",
    "\n",
    "# #2Rb inversion 2RL:18000000-32000000\n",
    "# #2La inversion 2RL:81545105-104545105\n",
    "\n",
    "#Get calls for mosquitoes collected on the final round of collections(Post-intervention). \n",
    "\n",
    "#If needed, you can split by net type (PBO/ Non PBO) and/or location (eastern/ western Uganda)\n",
    "sample_query = \"control_phase==['post']\"\n",
    "\n",
    "meta = ag3.sample_metadata(\n",
    "    sample_sets=sample_sets, \n",
    "    sample_query=sample_query,\n",
    ")\n",
    "\n",
    "# Randomly select sample IDs from the filtered metadata\n",
    "sample_subset = meta.sample(n=275, random_state=42).sample_id.to_list()\n",
    "\n",
    "# Create a formatted string for the sample_query parameter\n",
    "sample_query_subset = \"sample_id in ({})\".format(\", \".join(f\"'{id}'\" for id in sample_subset))\n",
    "\n",
    "#get calls from two chromosomes as we estimate inter-chromosome LD\n",
    "calls_1 = ag3.biallelic_snp_calls(region=['2RL:1-17990000', '2RL:32010000-81535105', '2RL:104555105-110909430'],\n",
    "                                  sample_sets=sample_sets,\n",
    "                                  sample_query=sample_query_subset,\n",
    "                                 )\n",
    "calls_2 = ag3.biallelic_snp_calls(region='3RL',\n",
    "                                  sample_sets=sample_sets,\n",
    "                                  sample_query=sample_query_subset,\n",
    "                                 )\n",
    "\n",
    "callset_1_post = calls_1['call_genotype']\n",
    "callset_2_post = calls_2['call_genotype']\n",
    "\n",
    "if np.any(callset_1_post['sample_id'] != callset_2_post['sample_id']):\n",
    "    raise Exception('Callsets 1 and 2 should contain the same samples')\n",
    "\n",
    "callset_1_post\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97afd318",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Get calls for mosquitoes collected on the baseline round of collections(Pre-intervention). \n",
    "\n",
    "sample_query = \"control_phase==['pre']\"\n",
    "\n",
    "meta = ag3.sample_metadata(\n",
    "    sample_sets=sample_sets, \n",
    "    sample_query=sample_query,\n",
    ")\n",
    "\n",
    "# Randomly select sample IDs from the filtered metadata\n",
    "sample_subset = meta.sample(n=275, random_state=42).sample_id.to_list()\n",
    "\n",
    "# Create a formatted string for the sample_query parameter\n",
    "sample_query_subset = \"sample_id in ({})\".format(\", \".join(f\"'{id}'\" for id in sample_subset))\n",
    "\n",
    "calls_1 = ag3.biallelic_snp_calls(region=['2RL:1-17990000', '2RL:32010000-81535105', '2RL:104555105-110909430'],\n",
    "                                  sample_sets=sample_sets,\n",
    "                                  sample_query=sample_query_subset,\n",
    "                                 )\n",
    "calls_2 = ag3.biallelic_snp_calls(region='3RL',\n",
    "                                  sample_sets=sample_sets,\n",
    "                                  sample_query=sample_query_subset,\n",
    "                                 )\n",
    "\n",
    "callset_1_pre = calls_1['call_genotype']\n",
    "callset_2_pre = calls_2['call_genotype']\n",
    "\n",
    "if np.any(callset_1_pre['sample_id'] != callset_2_pre['sample_id']):\n",
    "\traise Exception('Callsets 1 and 2 should contain the same samples')\n",
    "\n",
    "callset_1_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ac3c83",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "callset_1_post = callset_1_post.compute()\n",
    "callset_2_post = callset_2_post.compute()\n",
    "\n",
    "genotypes_1_post = np.sum(callset_1_post, 2)\n",
    "genotypes_2_post= np.sum(callset_2_post, 2)\n",
    "\n",
    "# Remove sites with missing calldata\n",
    "missing_filter_1 = np.any(genotypes_1_post < 0, 1)\n",
    "genotypes_1_post = genotypes_1_post[~missing_filter_1, :]\n",
    "missing_filter_2 = np.any(genotypes_2_post< 0, 1)\n",
    "genotypes_2_post= genotypes_2_post[~missing_filter_2, :]\n",
    "\n",
    "# Remove non_segregating loci\n",
    "segregating_filter_1 = np.logical_and(np.sum(genotypes_1_post, 1) > 0, np.sum(genotypes_1_post, 1) < (2*genotypes_1_post.shape[1]))\n",
    "genotypes_1_post = genotypes_1_post[segregating_filter_1, :].compute()\n",
    "segregating_filter_2 = np.logical_and(np.sum(genotypes_2_post, 1) > 0, np.sum(genotypes_2_post, 1) < (2*genotypes_2_post.shape[1]))\n",
    "genotypes_2_post= genotypes_2_post[segregating_filter_2, :].compute()\n",
    "\n",
    "g1_post, g2_post, rs_post = process_genotypes(genotypes_1_post, genotypes_2_post)\n",
    "\n",
    "g2_post.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84c082ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the results\n",
    "with open('Ne_allpop_post.pickle', 'wb') as f :\n",
    "\tpickle.dump((genotypes_1_post, genotypes_2_post, g1_post, g2_post,rs_post), f)\n",
    "\n",
    "# What is the variation in actual population size estimate?\n",
    "W = Weir_Ne(rs_post, genotypes_2_post.shape[1], c = 0.5)\n",
    "print(W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7bd51a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "callset_1_pre = callset_1_pre.compute()\n",
    "callset_2_pre = callset_2_pre.compute()\n",
    "\n",
    "genotypes_1_pre = np.sum(callset_1_pre, 2)\n",
    "genotypes_2_pre= np.sum(callset_2_pre, 2)\n",
    "\n",
    "# Remove sites with missing calldata\n",
    "missing_filter_1 = np.any(genotypes_1_pre < 0, 1)\n",
    "genotypes_1_pre = genotypes_1_pre[~missing_filter_1, :]\n",
    "missing_filter_2 = np.any(genotypes_2_pre< 0, 1)\n",
    "genotypes_2_pre= genotypes_2_pre[~missing_filter_2, :]\n",
    "\n",
    "# Remove non_segregating loci\n",
    "segregating_filter_1 = np.logical_and(np.sum(genotypes_1_pre, 1) > 0, np.sum(genotypes_1_pre, 1) < (2*genotypes_1_pre.shape[1]))\n",
    "genotypes_1_pre = genotypes_1_pre[segregating_filter_1, :].compute()\n",
    "segregating_filter_2 = np.logical_and(np.sum(genotypes_2_pre, 1) > 0, np.sum(genotypes_2_pre, 1) < (2*genotypes_2_pre.shape[1]))\n",
    "genotypes_2_pre= genotypes_2_pre[segregating_filter_2, :].compute()\n",
    "\n",
    "g1_pre, g2_pre, rs_pre = process_genotypes(genotypes_1_pre, genotypes_2_pre)\n",
    "\n",
    "g2_pre.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7592ed94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save the results\n",
    "with open('Ne_allpop_pre.pickle', 'wb') as f :\n",
    "\tpickle.dump((genotypes_1_pre, genotypes_2_pre, g1_pre, g2_pre, rs_pre), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bb90089",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load data from the pickle file\n",
    "# with open('Ne_allpop_pre.pickle', 'rb') as f:\n",
    "#  genotypes_1_pre, genotypes_2_pre, g1_pre, g2_pre, rs_pre= pickle.load(f)\n",
    "# with open('Ne_allpop_pre.pickle', 'rb') as c:\n",
    "#  genotypes_1_post, genotypes_2_post, g1_post, g2_post, rs_post= pickle.load(c)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69344322",
   "metadata": {},
   "outputs": [],
   "source": [
    "# What is the variation in actual population size estimate?\n",
    "W_pre = Weir_Ne(rs_pre , genotypes_2_post.shape[1], c = 0.5)\n",
    "print(W_pre)\n",
    "W_post = Weir_Ne(rs_post ,genotypes_2_post.shape[1], c = 0.5)\n",
    "print(W_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeb0c2d8-4f93-4198-b4d7-b04fe3889052",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# skip this step if you use the genotypes generated by process_genotype. only use if load genotypes_*\n",
    "\n",
    "# filter singletons and hets\n",
    "def filter_genotypes(genotypes_1, genotypes_2, sample_size = None, all_pairwise_thresh = 20000):\n",
    "\tif (genotypes_1.shape[1] != genotypes_2.shape[1]):\n",
    "\t\traise Exception('The two genotypes should have the same number of samples. ')\n",
    "\tif sample_size is None:\n",
    "\t\tg_full_1 = genotypes_1[:,:]\n",
    "\t\tg_full_2 = genotypes_2[:,:]\n",
    "\telse:\n",
    "\t\tg_full_1 = genotypes_1[:, :sample_size]\n",
    "\t\tg_full_2 = genotypes_2[:, :sample_size]\n",
    "\t# remove singletons\n",
    "\tsingleton_filter_1 = np.logical_and(np.sum(g_full_1, 1) > 1, np.sum(g_full_1, 1) < (2*g_full_1.shape[1] - 1))\n",
    "\tg_full_1 = g_full_1[singleton_filter_1, :]\n",
    "\tsingleton_filter_2 = np.logical_and(np.sum(g_full_2, 1) > 1, np.sum(g_full_2, 1) < (2*g_full_2.shape[1] - 1))\n",
    "\tg_full_2 = g_full_2[singleton_filter_2, :]\n",
    "\t# Remove sites where every sample is heterozygous, since this throws off the calculations,\n",
    "\t# and also these are probably dodgy sites anyway.\n",
    "\tfull_het_filter_1 = np.all(g_full_1 == 1, 1)\n",
    "\tg1 = g_full_1[~full_het_filter_1, :]\n",
    "\tfull_het_filter_2 = np.all(g_full_2 == 1, 1)\n",
    "\tg2 = g_full_2[~full_het_filter_2, :]\n",
    "\t\n",
    "\treturn(g1, g2)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4e4a462f",
   "metadata": {},
   "outputs": [],
   "source": [
    "g1_pre, g2_pre = filter_genotypes(genotypes_1_pre, genotypes_2_pre)\n",
    "g1_post, g2_post = filter_genotypes(genotypes_1_post, genotypes_2_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b401055",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract R-squared values from rs_pre and rs_post if not an array\n",
    "rs_pre_rsq = rs_pre#[-1]\n",
    "rs_post_rsq = rs_post#[-1]\n",
    "# Add a small value to all rsquared values\n",
    "if np.min(rs_pre_rsq) == 0 or np.min(rs_post_rsq) == 0:\n",
    "    small_value = 0.0000001  # or any small value you prefer\n",
    "rs_pre_rsq += small_value\n",
    "rs_post_rsq += small_value\n",
    "# Take the logarithm of R-squared values\n",
    "rs_pre_rsq_log = np.log10(rs_pre_rsq)\n",
    "rs_post_rsq_log = np.log10(rs_post_rsq)\n",
    "\n",
    "# Combine the log R-squared values with corresponding labels\n",
    "data = pd.DataFrame({'R-squared (log)': np.concatenate([rs_pre_rsq_log, rs_post_rsq_log]),\n",
    "                     'Group': ['rs_pre'] * len(rs_pre_rsq_log) + ['rs_post'] * len(rs_post_rsq_log)})\n",
    "\n",
    "# Set seaborn style and font scale\n",
    "sns.set(style=\"whitegrid\", font_scale=1.5)\n",
    "\n",
    "# Create a figure with specific dimensions and resolution\n",
    "plt.figure(figsize=(8, 6), dpi=300)\n",
    "\n",
    "# Define custom colors for each group\n",
    "colors = ['#1f77b4', '#ff7f0e']  # Blue and orange colors\n",
    "\n",
    "# Create a box plot using Seaborn with custom colors\n",
    "ax = sns.boxplot(x='Group', y='R-squared (log)', data=data, linewidth=2, hue='Group', palette=colors, legend=False)\n",
    "\n",
    "# Remove grid lines on the main plot area\n",
    "ax.grid(False)\n",
    "\n",
    "# Set axis labels and title\n",
    "#plt.title('Comparison of R-squared Values (rs_pre vs rs_post)', fontsize=14)\n",
    "plt.xlabel('Groups', fontsize=12)\n",
    "plt.ylabel('Log R-squared', fontsize=12)\n",
    "\n",
    "# Set tick label font sizes\n",
    "ax.tick_params(axis='x', labelsize=12)\n",
    "ax.tick_params(axis='y', labelsize=12)\n",
    "\n",
    "# Set linewidth of boxplot elements\n",
    "for box in ax.artists:\n",
    "    box.set_linewidth(2)\n",
    "\n",
    "# Save the plot \n",
    "plt.savefig('Ld_allpop.png', dpi=300, bbox_inches='tight')\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cd6efe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use logs squares to reduce skewness\n",
    "\n",
    "# R-squared values extracted from rs_pre and rs_post tuples\n",
    "rs_pre_rsq = rs_pre#[-1]\n",
    "rs_post_rsq = rs_post#[-1]\n",
    "\n",
    "# Check for negative values in rs_pre_rsq\n",
    "if np.any(rs_pre_rsq < 0):\n",
    "    print(\"There are negative values in rs_pre_rsq.\")\n",
    "else:\n",
    "    print(\"There are no negative values in rs_pre_rsq.\")\n",
    "\n",
    "# Check for zero values in rs_pre_rsq\n",
    "if np.any(rs_pre_rsq == 0):\n",
    "    print(\"There are zero values in rs_pre_rsq.\")\n",
    "else:\n",
    "    print(\"There are no zero values in rs_pre_rsq.\")\n",
    "\n",
    "# Check for negative values in rs_post_rsq\n",
    "if np.any(rs_post_rsq < 0):\n",
    "    print(\"There are negative values in rs_post_rsq.\")\n",
    "else:\n",
    "    print(\"There are no negative values in rs_post_rsq.\")\n",
    "\n",
    "# Check for zero values in rs_post_rsq\n",
    "if np.any(rs_post_rsq == 0):\n",
    "    print(\"There are zero values in rs_post_rsq.\")\n",
    "else:\n",
    "    print(\"There are no zero values in rs_post_rsq.\")\n",
    "\n",
    "# Calculate and print median of rs_pre_rsq\n",
    "median_rs_pre = np.median(rs_pre_rsq)\n",
    "print(\"Median of pre_rsq from Eastern Uganda:\", median_rs_pre)\n",
    "\n",
    "# Calculate and print median of rs_post_rsq\n",
    "median_rs_post = np.median(rs_post_rsq)\n",
    "print(\"Median of post_rsq from Eastern Uganda:\", median_rs_post)\n",
    "# Count the number of pairs with an r-squared value of 1\n",
    "pre_pairs_with_rsq_of_1 = np.sum(rs_pre_rsq > 0.99)\n",
    "\n",
    "print(\"Number of pre intervention pairs with an r-squared value of >0.99:\", pre_pairs_with_rsq_of_1)\n",
    "\n",
    "# Count the number of pairs with an r-squared value of 1\n",
    "post_pairs_with_rsq_of_1 = np.sum(rs_post_rsq > 0.99)\n",
    "\n",
    "print(\"Number of post intervention pairs with an r-squared value of >0.99:\", post_pairs_with_rsq_of_1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6151a05",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mannwhitneyu(rs_pre,rs_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ee696c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now write a function to get confidence intervals by picking a subset of the SNPs\n",
    "# and using a different subset each time\n",
    "def Weir_Ne_replication(call_set1, call_set2, N_subset, c = 0.5, n_replicates = 1000, p = 0.95):\n",
    "\tif call_set1.shape[1] != call_set2.shape[1]:\n",
    "\t\traise Exception('The two callsets need to have the same number of samples.')\n",
    "\tif N_subset > min([call_set1.shape[0], call_set2.shape[0]]):\n",
    "\t\traise Exception('N_subset cannot be larger than the smaller of the two genotype matrices.')\n",
    "\tS = call_set1.shape[1]\n",
    "\tunsorted_Ws = np.array([])\n",
    "\tfor i in range(n_replicates):\n",
    "\t\tprint(i)\n",
    "\t\trs_i = twochrom_r_squared(call_set1, call_set2, N_subset)\n",
    "\t\tunsorted_Ws = np.append(unsorted_Ws, Weir_Ne(rs_i, S, c))\n",
    "\tWs = np.sort(unsorted_Ws)\n",
    "\t# Work out the indices of the percentiles. Because of floating point issues, we need to dance around a bit here.\n",
    "\tmin_perc_index = n_replicates * round(1-p, 3) / 2\n",
    "\tmax_perc_index = int(np.ceil(n_replicates - min_perc_index) - 1)\n",
    "\tmin_perc_index = int(min_perc_index)\n",
    "\t# Now get the mean and confidence intervals\n",
    "\tW = np.mean(Ws)\n",
    "\tmin_perc = Ws[min_perc_index]\n",
    "\tmax_perc = Ws[max_perc_index]\n",
    "\t# Now report the mean and confidence intervals, and then all the boostrapped values\n",
    "\treturn(W, min_perc, max_perc, Ws)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "704e116e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Alternatively, we use a fixed subset of SNPs, and we boostrap over samples. As with\n",
    "# the non-bootstrapping function we don't quite do the same thing if we set a subset\n",
    "# and if we don't. If don't stipulate a subset, we use all pairwise combinations of\n",
    "# loci between the two chromosomes. If we do, we just assign each locus on chrom 1 to\n",
    "# a given locus on chrom 2.\n",
    "def Weir_Ne_bootstrap_by_sample(call_set1, call_set2, c = 0.5, subsample_size = 247, N_subset = 100000, n_bootstrap = 1000, p = 0.95):\n",
    "\tif call_set1.shape[1] != call_set2.shape[1]:\n",
    "\t\traise Exception('The two callsets need to have the same number of samples.')\n",
    "\tS = call_set1.shape[1]\n",
    "\tif N_subset is not None and N_subset <= min([call_set1.shape[0], call_set2.shape[0]]):\n",
    "\t\t# Reduce the call_sets to a subset of them\n",
    "\t\tsubset1 = np.array(random.sample(range(call_set1.shape[0]), N_subset))\n",
    "\t\tcall_set1 = call_set1[subset1, :]\n",
    "\t\tsubset2 = np.array(random.sample(range(call_set2.shape[0]), N_subset))\n",
    "\t\tcall_set2 = call_set2[subset2, :]\n",
    "\t# The line below will work differently depending on the value of N_subset. If it\n",
    "\t# was set to None, then it will calculate rs based on all pairwise combinations\n",
    "\t# of loci. If N_subset was set to a sensible value (no larger than the smaller of\n",
    "\t# the two callsets), then we calculate rs while setting the subset to be equal to\n",
    "\t# the callset sizes. This means that the function will assign each SNP in call_set1\n",
    "\t# to a SNP in call_set2. That random allocation will be different for each bootstrap\n",
    "\t# replication, but I think that's fine.\n",
    "\trs = twochrom_r_squared(call_set1, call_set2, N_subset)\n",
    "\tW = Weir_Ne(rs, S, c)\n",
    "\tW_bs = np.array([])\n",
    "\tfor i in range(n_bootstrap):\n",
    "\t\tprint(i)\n",
    "\t\ts = np.random.choice(range(S), subsample_size, replace = False)\n",
    "\t\tc1 = call_set1[:, s]\n",
    "\t\tc2 = call_set2[:, s]\n",
    "\t\t#\n",
    "\t\tsingleton_filter_1 = np.logical_and(np.sum(c1, 1) > 1, np.sum(c1, 1) < (2*c1.shape[1] - 1))\n",
    "\t\tc1 = c1[singleton_filter_1, :]\n",
    "\t\tsingleton_filter_2 = np.logical_and(np.sum(c2, 1) > 1, np.sum(c2, 1) < (2*c2.shape[1] - 1))\n",
    "\t\tc2 = c2[singleton_filter_2, :]\n",
    "\t\t# Remove sites where every sample is heterozygous, since this throws off the calculations,\n",
    "\t\t# and also these are probably dodgy sites anyway.\n",
    "\t\tfull_het_filter_1 = np.all(c1 == 1, 1)\n",
    "\t\tc1 = c1[~full_het_filter_1, :]\n",
    "\t\tfull_het_filter_2 = np.all(c2 == 1, 1)\n",
    "\t\tc2 = c2[~full_het_filter_2, :]\n",
    "\t\t#\n",
    "\t\trs_i = twochrom_r_squared(c1, c2, np.min([c1.shape[0], c2.shape[0]]))\n",
    "\t\tW_bs = np.append(W_bs, Weir_Ne(rs_i, subsample_size, c))\n",
    "\t# Now get the differences between the bootstrap estimates and the observed value.\n",
    "\tsorted_W_bs = np.sort(W_bs)\n",
    "\t# Work out the indices of the percentiles. Because of floating point issues, we need to dance around a bit here.\n",
    "\tmin_perc_index = n_bootstrap * round(1-p, 3) / 2\n",
    "\tmax_perc_index = int(np.ceil(n_bootstrap - min_perc_index) - 1)\n",
    "\tmin_perc_index = int(min_perc_index)\n",
    "\t# Now get the confidence intervals\n",
    "\tmin_perc = sorted_W_bs[min_perc_index]\n",
    "\tmax_perc = sorted_W_bs[max_perc_index]\n",
    "\t# Now report the mean and confidence intervals, and then all the boostrapped values\n",
    "\treturn(W, min_perc, max_perc, sorted_W_bs)\n",
    "\n",
    "#num_loci = min([g1.shape[0], g2.shape[0]])\n",
    "#conf_interval_replicates_largepop_nocrash += [Weir_Ne_replication(g1, g2], num_loci)]\n",
    "#num_loci = min([g1.shape[0], g2.shape[0]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee5be1b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates1_post = Weir_Ne_replication(g1_post, g2_post, N_subset = 1000)\n",
    "replicates1_post"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9babf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates1_pre = Weir_Ne_replication(g1_pre, g2_pre, N_subset = 1000)\n",
    "replicates1_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2f483c",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates2_post = Weir_Ne_bootstrap_by_sample(g1_post, g2_post)\n",
    "replicates2_post\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2a2c18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "replicates2_pre = Weir_Ne_bootstrap_by_sample(g1_pre, g2_pre)\n",
    "replicates2_pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bf6c099d",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('r2_allpop_pre.pickle', 'wb') as f :\n",
    "\tpickle.dump((replicates2_pre), f)\n",
    "with open('r2_allpop_post.pickle', 'wb') as f :\n",
    "\tpickle.dump((replicates2_post), f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1984e035",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the 5th percentile, median, and 95th percentile\n",
    "percentile_5 = np.percentile(rs_pre, 5)\n",
    "median = np.median(rs_pre)\n",
    "percentile_95 = np.percentile(rs_pre, 95)\n",
    "\n",
    "print(\"5th Percentile:\", percentile_5)\n",
    "print(\"Median:\", median)\n",
    "print(\"95th Percentile:\", percentile_95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05b3c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.mannwhitneyu(rs_pre,rs_post)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b437de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use a large value to represent infinite Ne\n",
    "r_post = replicates2_post[3]\n",
    "r_post[np.isinf(r_post)] = 5000000\n",
    "r_pre = replicates2_pre[3]\n",
    "r_pre[np.isinf(r_pre)] = 5000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f44c68a5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# Log-transform the data\n",
    "log_r_post = np.log10(r_post)\n",
    "log_r_pre = np.log10(r_pre)\n",
    "\n",
    "# Plotting side-by-side histograms\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(log_r_post, bins=30, density=True, alpha=0.7, color='blue', edgecolor='black')\n",
    "plt.xlabel('log(Ne_post) estimates')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Npbo Post-LLINEUP trial Ne estimates in Eastern Uganda')\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.hist(log_r_pre, bins=30, density=True, alpha=0.7, color='green', edgecolor='black')\n",
    "plt.xlabel('log(Ne_pre) estimates')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Npbo Pre-LLINEUP trial Ne estimates in Eastern Uganda')\n",
    "\n",
    "# Adjust layout for better visualization\n",
    "plt.tight_layout()\n",
    "#save\n",
    "plt.savefig('ne_npbo_east_hist.png')\n",
    "# Show the plot\n",
    "plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaard",
   "language": "python",
   "name": "gaard"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
