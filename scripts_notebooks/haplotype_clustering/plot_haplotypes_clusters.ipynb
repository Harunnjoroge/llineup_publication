{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time # required to output the time at which the script was run\n",
    "from sys import stdout # this import is need to flush python output to the stdout (instead of leaving it\n",
    "# in the buffer\n",
    "from sys import argv # this import is needed in order for the script to handle command line arguments\n",
    "import socket # this import allows us to access the name of the computer that the script is being run on\n",
    "import re\n",
    "from os import system\n",
    "import malariagen_data\n",
    "import allel\n",
    "import scipy.cluster\n",
    "from scipy import stats\n",
    "from scipy.stats import spearmanr\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import seaborn as sns\n",
    "import matplotlib.patches as mpatches\n",
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_sets= [\"1288-VO-UG-DONNELLY-VMF00168\",\"1288-VO-UG-DONNELLY-VMF00219\"]\n",
    "sample_query= (\"aim_species == 'gambiae'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": "(function(root) {\n  function now() {\n    return new Date();\n  }\n\n  const force = true;\n\n  if (typeof root._bokeh_onload_callbacks === \"undefined\" || force === true) {\n    root._bokeh_onload_callbacks = [];\n    root._bokeh_is_loading = undefined;\n  }\n\nconst JS_MIME_TYPE = 'application/javascript';\n  const HTML_MIME_TYPE = 'text/html';\n  const EXEC_MIME_TYPE = 'application/vnd.bokehjs_exec.v0+json';\n  const CLASS_NAME = 'output_bokeh rendered_html';\n\n  /**\n   * Render data to the DOM node\n   */\n  function render(props, node) {\n    const script = document.createElement(\"script\");\n    node.appendChild(script);\n  }\n\n  /**\n   * Handle when an output is cleared or removed\n   */\n  function handleClearOutput(event, handle) {\n    const cell = handle.cell;\n\n    const id = cell.output_area._bokeh_element_id;\n    const server_id = cell.output_area._bokeh_server_id;\n    // Clean up Bokeh references\n    if (id != null && id in Bokeh.index) {\n      Bokeh.index[id].model.document.clear();\n      delete Bokeh.index[id];\n    }\n\n    if (server_id !== undefined) {\n      // Clean up Bokeh references\n      const cmd_clean = \"from bokeh.io.state import curstate; print(curstate().uuid_to_server['\" + server_id + \"'].get_sessions()[0].document.roots[0]._id)\";\n      cell.notebook.kernel.execute(cmd_clean, {\n        iopub: {\n          output: function(msg) {\n            const id = msg.content.text.trim();\n            if (id in Bokeh.index) {\n              Bokeh.index[id].model.document.clear();\n              delete Bokeh.index[id];\n            }\n          }\n        }\n      });\n      // Destroy server and session\n      const cmd_destroy = \"import bokeh.io.notebook as ion; ion.destroy_server('\" + server_id + \"')\";\n      cell.notebook.kernel.execute(cmd_destroy);\n    }\n  }\n\n  /**\n   * Handle when a new output is added\n   */\n  function handleAddOutput(event, handle) {\n    const output_area = handle.output_area;\n    const output = handle.output;\n\n    // limit handleAddOutput to display_data with EXEC_MIME_TYPE content only\n    if ((output.output_type != \"display_data\") || (!Object.prototype.hasOwnProperty.call(output.data, EXEC_MIME_TYPE))) {\n      return\n    }\n\n    const toinsert = output_area.element.find(\".\" + CLASS_NAME.split(' ')[0]);\n\n    if (output.metadata[EXEC_MIME_TYPE][\"id\"] !== undefined) {\n      toinsert[toinsert.length - 1].firstChild.textContent = output.data[JS_MIME_TYPE];\n      // store reference to embed id on output_area\n      output_area._bokeh_element_id = output.metadata[EXEC_MIME_TYPE][\"id\"];\n    }\n    if (output.metadata[EXEC_MIME_TYPE][\"server_id\"] !== undefined) {\n      const bk_div = document.createElement(\"div\");\n      bk_div.innerHTML = output.data[HTML_MIME_TYPE];\n      const script_attrs = bk_div.children[0].attributes;\n      for (let i = 0; i < script_attrs.length; i++) {\n        toinsert[toinsert.length - 1].firstChild.setAttribute(script_attrs[i].name, script_attrs[i].value);\n        toinsert[toinsert.length - 1].firstChild.textContent = bk_div.children[0].textContent\n      }\n      // store reference to server id on output_area\n      output_area._bokeh_server_id = output.metadata[EXEC_MIME_TYPE][\"server_id\"];\n    }\n  }\n\n  function register_renderer(events, OutputArea) {\n\n    function append_mime(data, metadata, element) {\n      // create a DOM node to render to\n      const toinsert = this.create_output_subarea(\n        metadata,\n        CLASS_NAME,\n        EXEC_MIME_TYPE\n      );\n      this.keyboard_manager.register_events(toinsert);\n      // Render to node\n      const props = {data: data, metadata: metadata[EXEC_MIME_TYPE]};\n      render(props, toinsert[toinsert.length - 1]);\n      element.append(toinsert);\n      return toinsert\n    }\n\n    /* Handle when an output is cleared or removed */\n    events.on('clear_output.CodeCell', handleClearOutput);\n    events.on('delete.Cell', handleClearOutput);\n\n    /* Handle when a new output is added */\n    events.on('output_added.OutputArea', handleAddOutput);\n\n    /**\n     * Register the mime type and append_mime function with output_area\n     */\n    OutputArea.prototype.register_mime_type(EXEC_MIME_TYPE, append_mime, {\n      /* Is output safe? */\n      safe: true,\n      /* Index of renderer in `output_area.display_order` */\n      index: 0\n    });\n  }\n\n  // register the mime type if in Jupyter Notebook environment and previously unregistered\n  if (root.Jupyter !== undefined) {\n    const events = require('base/js/events');\n    const OutputArea = require('notebook/js/outputarea').OutputArea;\n\n    if (OutputArea.prototype.mime_types().indexOf(EXEC_MIME_TYPE) == -1) {\n      register_renderer(events, OutputArea);\n    }\n  }\n  if (typeof (root._bokeh_timeout) === \"undefined\" || force === true) {\n    root._bokeh_timeout = Date.now() + 5000;\n    root._bokeh_failed_load = false;\n  }\n\n  const NB_LOAD_WARNING = {'data': {'text/html':\n     \"<div style='background-color: #fdd'>\\n\"+\n     \"<p>\\n\"+\n     \"BokehJS does not appear to have successfully loaded. If loading BokehJS from CDN, this \\n\"+\n     \"may be due to a slow or bad network connection. Possible fixes:\\n\"+\n     \"</p>\\n\"+\n     \"<ul>\\n\"+\n     \"<li>re-rerun `output_notebook()` to attempt to load from CDN again, or</li>\\n\"+\n     \"<li>use INLINE resources instead, as so:</li>\\n\"+\n     \"</ul>\\n\"+\n     \"<code>\\n\"+\n     \"from bokeh.resources import INLINE\\n\"+\n     \"output_notebook(resources=INLINE)\\n\"+\n     \"</code>\\n\"+\n     \"</div>\"}};\n\n  function display_loaded() {\n    const el = document.getElementById(null);\n    if (el != null) {\n      el.textContent = \"BokehJS is loading...\";\n    }\n    if (root.Bokeh !== undefined) {\n      if (el != null) {\n        el.textContent = \"BokehJS \" + root.Bokeh.version + \" successfully loaded.\";\n      }\n    } else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(display_loaded, 100)\n    }\n  }\n\n  function run_callbacks() {\n    try {\n      root._bokeh_onload_callbacks.forEach(function(callback) {\n        if (callback != null)\n          callback();\n      });\n    } finally {\n      delete root._bokeh_onload_callbacks\n    }\n    console.debug(\"Bokeh: all callbacks have finished\");\n  }\n\n  function load_libs(css_urls, js_urls, callback) {\n    if (css_urls == null) css_urls = [];\n    if (js_urls == null) js_urls = [];\n\n    root._bokeh_onload_callbacks.push(callback);\n    if (root._bokeh_is_loading > 0) {\n      console.debug(\"Bokeh: BokehJS is being loaded, scheduling callback at\", now());\n      return null;\n    }\n    if (js_urls == null || js_urls.length === 0) {\n      run_callbacks();\n      return null;\n    }\n    console.debug(\"Bokeh: BokehJS not loaded, scheduling load and callback at\", now());\n    root._bokeh_is_loading = css_urls.length + js_urls.length;\n\n    function on_load() {\n      root._bokeh_is_loading--;\n      if (root._bokeh_is_loading === 0) {\n        console.debug(\"Bokeh: all BokehJS libraries/stylesheets loaded\");\n        run_callbacks()\n      }\n    }\n\n    function on_error(url) {\n      console.error(\"failed to load \" + url);\n    }\n\n    for (let i = 0; i < css_urls.length; i++) {\n      const url = css_urls[i];\n      const element = document.createElement(\"link\");\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.rel = \"stylesheet\";\n      element.type = \"text/css\";\n      element.href = url;\n      console.debug(\"Bokeh: injecting link tag for BokehJS stylesheet: \", url);\n      document.body.appendChild(element);\n    }\n\n    for (let i = 0; i < js_urls.length; i++) {\n      const url = js_urls[i];\n      const element = document.createElement('script');\n      element.onload = on_load;\n      element.onerror = on_error.bind(null, url);\n      element.async = false;\n      element.src = url;\n      console.debug(\"Bokeh: injecting script tag for BokehJS library: \", url);\n      document.head.appendChild(element);\n    }\n  };\n\n  function inject_raw_css(css) {\n    const element = document.createElement(\"style\");\n    element.appendChild(document.createTextNode(css));\n    document.body.appendChild(element);\n  }\n\n  const js_urls = [\"https://cdn.bokeh.org/bokeh/release/bokeh-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-gl-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-widgets-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-tables-2.4.3.min.js\", \"https://cdn.bokeh.org/bokeh/release/bokeh-mathjax-2.4.3.min.js\"];\n  const css_urls = [];\n\n  const inline_js = [    function(Bokeh) {\n      Bokeh.set_log_level(\"info\");\n    },\nfunction(Bokeh) {\n    }\n  ];\n\n  function run_inline_js() {\n    if (root.Bokeh !== undefined || force === true) {\n          for (let i = 0; i < inline_js.length; i++) {\n      inline_js[i].call(root, root.Bokeh);\n    }\n} else if (Date.now() < root._bokeh_timeout) {\n      setTimeout(run_inline_js, 100);\n    } else if (!root._bokeh_failed_load) {\n      console.log(\"Bokeh: BokehJS failed to load within specified timeout.\");\n      root._bokeh_failed_load = true;\n    } else if (force !== true) {\n      const cell = $(document.getElementById(null)).parents('.cell').data().cell;\n      cell.output_area.append_execute_result(NB_LOAD_WARNING)\n    }\n  }\n\n  if (root._bokeh_is_loading === 0) {\n    console.debug(\"Bokeh: BokehJS loaded, going straight to plotting\");\n    run_inline_js();\n  } else {\n    load_libs(css_urls, js_urls, function() {\n      console.debug(\"Bokeh: BokehJS plotting callback run at\", now());\n      run_inline_js();\n    });\n  }\n}(window));",
      "application/vnd.bokehjs_load.v0+json": ""
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1222, 57)                           \n",
      "                                     \r"
     ]
    }
   ],
   "source": [
    "#load malariagen data, metadata and edit\n",
    "\n",
    "#####################################################################################################################################################\n",
    "ag3 = malariagen_data.Ag3(pre = True)\n",
    "# Initially, we don't use a sample query\n",
    "meta = ag3.sample_metadata(\n",
    "    sample_sets=sample_sets, \n",
    ")\n",
    "# Remove decimal and numbers after it in the \"partner_sample_id\" column not present in our meta data\n",
    "meta['partner_sample_id'] = meta['partner_sample_id'].str.split('.').str[0]\n",
    "print(meta.shape)\n",
    "\n",
    "# Convert \"partner_sample_id\" column in meta to float64 to match llineup_meta dtype\n",
    "meta['partner_sample_id'] = pd.to_numeric(meta['partner_sample_id'])\n",
    "#llineup trial metadata\n",
    "llineup_meta = pd.read_csv('./llineup_publication/Data/ento_geno_plasmo_data.csv',\n",
    "                       index_col = 0,\n",
    "                      ).query(\"species=='An. gambiae'\")\n",
    "llineup_meta= llineup_meta.drop_duplicates(subset=['wgs.sample.id'])\n",
    "\n",
    "wrong_row = (llineup_meta['wgs.sample.id'] == '107506144173') & (llineup_meta['Sample.identifier'] == '60')\n",
    "llineup_meta.loc[wrong_row, 'wgs.sample.id'] = '107506144500'\n",
    "llineup_meta.set_index('wgs.sample.id', inplace = True)\n",
    "llineup_meta = llineup_meta.reindex(index=meta['partner_sample_id'])#match order in malariagen meta data\n",
    "llineup_meta=llineup_meta.reset_index()\n",
    "llineup_meta['sample_id'] =meta['sample_id'] #create sample id to merge with ag3 metadata\n",
    "sample_query_filter = (meta['aim_species'] == 'gambiae') & (meta['sex_call'] == 'F')\n",
    "\n",
    "llineup_meta['control_phase'] = llineup_meta['RND'].map({1: 'pre', 5: 'post'}).fillna('intermediate') #not needed but include if you want to plot control_phase\n",
    "\n",
    "llineup_meta.rename(columns={'LLIN.actual':'llin_actual'}, inplace = True)\n",
    "llineup_meta.dropna(subset=['Location'], inplace=True)\n",
    "ag3.add_extra_metadata(llineup_meta)\n",
    "# Re-download the metadata with the additional columns\n",
    "meta = ag3.sample_metadata(\n",
    "    sample_sets=sample_sets,\n",
    "    sample_query=sample_query \n",
    ")\n",
    "\n",
    "meta.dropna(subset=['Location'], inplace=True)\n",
    "meta.set_index('sample_id', inplace=True)\n",
    "\n",
    "#meta = meta[meta['RND'].isin([1,5]) & (meta['llin_actual']=='Non-PBO LLIN')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>partner_sample_id_x</th>\n",
       "      <th>contributor</th>\n",
       "      <th>country</th>\n",
       "      <th>location</th>\n",
       "      <th>year</th>\n",
       "      <th>month</th>\n",
       "      <th>latitude</th>\n",
       "      <th>longitude</th>\n",
       "      <th>sex_call</th>\n",
       "      <th>sample_set</th>\n",
       "      <th>...</th>\n",
       "      <th>survey</th>\n",
       "      <th>LLIN.intend</th>\n",
       "      <th>llin_actual</th>\n",
       "      <th>Net.intend</th>\n",
       "      <th>Net.actual</th>\n",
       "      <th>Wave</th>\n",
       "      <th>Location</th>\n",
       "      <th>subregions</th>\n",
       "      <th>species</th>\n",
       "      <th>control_phase</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sample_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>VBS50295-6645STDY11194062</th>\n",
       "      <td>109410010012</td>\n",
       "      <td>Martin Donnelly</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>9410</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>1.075</td>\n",
       "      <td>33.390</td>\n",
       "      <td>F</td>\n",
       "      <td>1288-VO-UG-DONNELLY-VMF00168</td>\n",
       "      <td>...</td>\n",
       "      <td>baseline</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PermaNet-3</td>\n",
       "      <td>PermaNet-3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>East</td>\n",
       "      <td>East Central</td>\n",
       "      <td>An. gambiae</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS50296-6645STDY11194063</th>\n",
       "      <td>109410010006</td>\n",
       "      <td>Martin Donnelly</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>9410</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>1.075</td>\n",
       "      <td>33.390</td>\n",
       "      <td>F</td>\n",
       "      <td>1288-VO-UG-DONNELLY-VMF00168</td>\n",
       "      <td>...</td>\n",
       "      <td>baseline</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PermaNet-3</td>\n",
       "      <td>PermaNet-3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>East</td>\n",
       "      <td>East Central</td>\n",
       "      <td>An. gambiae</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS50297-6645STDY11194064</th>\n",
       "      <td>109410010005</td>\n",
       "      <td>Martin Donnelly</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>9410</td>\n",
       "      <td>2017</td>\n",
       "      <td>5</td>\n",
       "      <td>1.075</td>\n",
       "      <td>33.390</td>\n",
       "      <td>F</td>\n",
       "      <td>1288-VO-UG-DONNELLY-VMF00168</td>\n",
       "      <td>...</td>\n",
       "      <td>baseline</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PermaNet-3</td>\n",
       "      <td>PermaNet-3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>East</td>\n",
       "      <td>East Central</td>\n",
       "      <td>An. gambiae</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS50298-6645STDY11194065</th>\n",
       "      <td>109410010001</td>\n",
       "      <td>Martin Donnelly</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>9410</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>1.075</td>\n",
       "      <td>33.390</td>\n",
       "      <td>F</td>\n",
       "      <td>1288-VO-UG-DONNELLY-VMF00168</td>\n",
       "      <td>...</td>\n",
       "      <td>baseline</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PermaNet-3</td>\n",
       "      <td>PermaNet-3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>East</td>\n",
       "      <td>East Central</td>\n",
       "      <td>An. gambiae</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS50299-6645STDY11194066</th>\n",
       "      <td>109410010024</td>\n",
       "      <td>Martin Donnelly</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>9410</td>\n",
       "      <td>2017</td>\n",
       "      <td>6</td>\n",
       "      <td>1.075</td>\n",
       "      <td>33.390</td>\n",
       "      <td>F</td>\n",
       "      <td>1288-VO-UG-DONNELLY-VMF00168</td>\n",
       "      <td>...</td>\n",
       "      <td>baseline</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PermaNet-3</td>\n",
       "      <td>PermaNet-3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>East</td>\n",
       "      <td>East Central</td>\n",
       "      <td>An. gambiae</td>\n",
       "      <td>pre</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS65916-6645STDY13044822</th>\n",
       "      <td>506806007004.000</td>\n",
       "      <td>Martin Donnelly</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>6806</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2.194</td>\n",
       "      <td>33.722</td>\n",
       "      <td>F</td>\n",
       "      <td>1288-VO-UG-DONNELLY-VMF00219</td>\n",
       "      <td>...</td>\n",
       "      <td>25 months</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>OlysetPlus</td>\n",
       "      <td>OlysetPlus</td>\n",
       "      <td>2.0</td>\n",
       "      <td>East</td>\n",
       "      <td>Mid-Eastern</td>\n",
       "      <td>An. gambiae</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS65918-6645STDY13044824</th>\n",
       "      <td>506806007006.000</td>\n",
       "      <td>Martin Donnelly</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>6806</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2.194</td>\n",
       "      <td>33.722</td>\n",
       "      <td>F</td>\n",
       "      <td>1288-VO-UG-DONNELLY-VMF00219</td>\n",
       "      <td>...</td>\n",
       "      <td>25 months</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>OlysetPlus</td>\n",
       "      <td>OlysetPlus</td>\n",
       "      <td>2.0</td>\n",
       "      <td>East</td>\n",
       "      <td>Mid-Eastern</td>\n",
       "      <td>An. gambiae</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS65919-6645STDY13044825</th>\n",
       "      <td>506807032001.000</td>\n",
       "      <td>Martin Donnelly</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>6807</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2.239</td>\n",
       "      <td>33.577</td>\n",
       "      <td>F</td>\n",
       "      <td>1288-VO-UG-DONNELLY-VMF00219</td>\n",
       "      <td>...</td>\n",
       "      <td>25 months</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>OlysetPlus</td>\n",
       "      <td>OlysetPlus</td>\n",
       "      <td>2.0</td>\n",
       "      <td>East</td>\n",
       "      <td>Mid-Eastern</td>\n",
       "      <td>An. gambiae</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS65920-6645STDY13044826</th>\n",
       "      <td>506807032002.000</td>\n",
       "      <td>Martin Donnelly</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>6807</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2.239</td>\n",
       "      <td>33.577</td>\n",
       "      <td>F</td>\n",
       "      <td>1288-VO-UG-DONNELLY-VMF00219</td>\n",
       "      <td>...</td>\n",
       "      <td>25 months</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>OlysetPlus</td>\n",
       "      <td>OlysetPlus</td>\n",
       "      <td>2.0</td>\n",
       "      <td>East</td>\n",
       "      <td>Mid-Eastern</td>\n",
       "      <td>An. gambiae</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>VBS65921-6645STDY13044827</th>\n",
       "      <td>506807032003.000</td>\n",
       "      <td>Martin Donnelly</td>\n",
       "      <td>Uganda</td>\n",
       "      <td>6807</td>\n",
       "      <td>2019</td>\n",
       "      <td>5</td>\n",
       "      <td>2.239</td>\n",
       "      <td>33.577</td>\n",
       "      <td>F</td>\n",
       "      <td>1288-VO-UG-DONNELLY-VMF00219</td>\n",
       "      <td>...</td>\n",
       "      <td>25 months</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>PBO LLIN</td>\n",
       "      <td>OlysetPlus</td>\n",
       "      <td>OlysetPlus</td>\n",
       "      <td>2.0</td>\n",
       "      <td>East</td>\n",
       "      <td>Mid-Eastern</td>\n",
       "      <td>An. gambiae</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>238 rows × 96 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          partner_sample_id_x      contributor country  \\\n",
       "sample_id                                                                \n",
       "VBS50295-6645STDY11194062        109410010012  Martin Donnelly  Uganda   \n",
       "VBS50296-6645STDY11194063        109410010006  Martin Donnelly  Uganda   \n",
       "VBS50297-6645STDY11194064        109410010005  Martin Donnelly  Uganda   \n",
       "VBS50298-6645STDY11194065        109410010001  Martin Donnelly  Uganda   \n",
       "VBS50299-6645STDY11194066        109410010024  Martin Donnelly  Uganda   \n",
       "...                                       ...              ...     ...   \n",
       "VBS65916-6645STDY13044822    506806007004.000  Martin Donnelly  Uganda   \n",
       "VBS65918-6645STDY13044824    506806007006.000  Martin Donnelly  Uganda   \n",
       "VBS65919-6645STDY13044825    506807032001.000  Martin Donnelly  Uganda   \n",
       "VBS65920-6645STDY13044826    506807032002.000  Martin Donnelly  Uganda   \n",
       "VBS65921-6645STDY13044827    506807032003.000  Martin Donnelly  Uganda   \n",
       "\n",
       "                          location  year  month  latitude  longitude sex_call  \\\n",
       "sample_id                                                                       \n",
       "VBS50295-6645STDY11194062     9410  2017      5     1.075     33.390        F   \n",
       "VBS50296-6645STDY11194063     9410  2017      5     1.075     33.390        F   \n",
       "VBS50297-6645STDY11194064     9410  2017      5     1.075     33.390        F   \n",
       "VBS50298-6645STDY11194065     9410  2017      6     1.075     33.390        F   \n",
       "VBS50299-6645STDY11194066     9410  2017      6     1.075     33.390        F   \n",
       "...                            ...   ...    ...       ...        ...      ...   \n",
       "VBS65916-6645STDY13044822     6806  2019      5     2.194     33.722        F   \n",
       "VBS65918-6645STDY13044824     6806  2019      5     2.194     33.722        F   \n",
       "VBS65919-6645STDY13044825     6807  2019      5     2.239     33.577        F   \n",
       "VBS65920-6645STDY13044826     6807  2019      5     2.239     33.577        F   \n",
       "VBS65921-6645STDY13044827     6807  2019      5     2.239     33.577        F   \n",
       "\n",
       "                                             sample_set  ...     survey  \\\n",
       "sample_id                                                ...              \n",
       "VBS50295-6645STDY11194062  1288-VO-UG-DONNELLY-VMF00168  ...   baseline   \n",
       "VBS50296-6645STDY11194063  1288-VO-UG-DONNELLY-VMF00168  ...   baseline   \n",
       "VBS50297-6645STDY11194064  1288-VO-UG-DONNELLY-VMF00168  ...   baseline   \n",
       "VBS50298-6645STDY11194065  1288-VO-UG-DONNELLY-VMF00168  ...   baseline   \n",
       "VBS50299-6645STDY11194066  1288-VO-UG-DONNELLY-VMF00168  ...   baseline   \n",
       "...                                                 ...  ...        ...   \n",
       "VBS65916-6645STDY13044822  1288-VO-UG-DONNELLY-VMF00219  ...  25 months   \n",
       "VBS65918-6645STDY13044824  1288-VO-UG-DONNELLY-VMF00219  ...  25 months   \n",
       "VBS65919-6645STDY13044825  1288-VO-UG-DONNELLY-VMF00219  ...  25 months   \n",
       "VBS65920-6645STDY13044826  1288-VO-UG-DONNELLY-VMF00219  ...  25 months   \n",
       "VBS65921-6645STDY13044827  1288-VO-UG-DONNELLY-VMF00219  ...  25 months   \n",
       "\n",
       "                           LLIN.intend llin_actual  Net.intend  Net.actual  \\\n",
       "sample_id                                                                    \n",
       "VBS50295-6645STDY11194062     PBO LLIN    PBO LLIN  PermaNet-3  PermaNet-3   \n",
       "VBS50296-6645STDY11194063     PBO LLIN    PBO LLIN  PermaNet-3  PermaNet-3   \n",
       "VBS50297-6645STDY11194064     PBO LLIN    PBO LLIN  PermaNet-3  PermaNet-3   \n",
       "VBS50298-6645STDY11194065     PBO LLIN    PBO LLIN  PermaNet-3  PermaNet-3   \n",
       "VBS50299-6645STDY11194066     PBO LLIN    PBO LLIN  PermaNet-3  PermaNet-3   \n",
       "...                                ...         ...         ...         ...   \n",
       "VBS65916-6645STDY13044822     PBO LLIN    PBO LLIN  OlysetPlus  OlysetPlus   \n",
       "VBS65918-6645STDY13044824     PBO LLIN    PBO LLIN  OlysetPlus  OlysetPlus   \n",
       "VBS65919-6645STDY13044825     PBO LLIN    PBO LLIN  OlysetPlus  OlysetPlus   \n",
       "VBS65920-6645STDY13044826     PBO LLIN    PBO LLIN  OlysetPlus  OlysetPlus   \n",
       "VBS65921-6645STDY13044827     PBO LLIN    PBO LLIN  OlysetPlus  OlysetPlus   \n",
       "\n",
       "                          Wave  Location    subregions      species  \\\n",
       "sample_id                                                             \n",
       "VBS50295-6645STDY11194062  3.0      East  East Central  An. gambiae   \n",
       "VBS50296-6645STDY11194063  3.0      East  East Central  An. gambiae   \n",
       "VBS50297-6645STDY11194064  3.0      East  East Central  An. gambiae   \n",
       "VBS50298-6645STDY11194065  3.0      East  East Central  An. gambiae   \n",
       "VBS50299-6645STDY11194066  3.0      East  East Central  An. gambiae   \n",
       "...                        ...       ...           ...          ...   \n",
       "VBS65916-6645STDY13044822  2.0      East   Mid-Eastern  An. gambiae   \n",
       "VBS65918-6645STDY13044824  2.0      East   Mid-Eastern  An. gambiae   \n",
       "VBS65919-6645STDY13044825  2.0      East   Mid-Eastern  An. gambiae   \n",
       "VBS65920-6645STDY13044826  2.0      East   Mid-Eastern  An. gambiae   \n",
       "VBS65921-6645STDY13044827  2.0      East   Mid-Eastern  An. gambiae   \n",
       "\n",
       "                           control_phase  \n",
       "sample_id                                 \n",
       "VBS50295-6645STDY11194062            pre  \n",
       "VBS50296-6645STDY11194063            pre  \n",
       "VBS50297-6645STDY11194064            pre  \n",
       "VBS50298-6645STDY11194065            pre  \n",
       "VBS50299-6645STDY11194066            pre  \n",
       "...                                  ...  \n",
       "VBS65916-6645STDY13044822           post  \n",
       "VBS65918-6645STDY13044824           post  \n",
       "VBS65919-6645STDY13044825           post  \n",
       "VBS65920-6645STDY13044826           post  \n",
       "VBS65921-6645STDY13044827           post  \n",
       "\n",
       "[238 rows x 96 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "meta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c575ee47a1ca4bd19a74c45f86086126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Load CNV HMM data:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7148c220b114b69b15deb6af22312dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute modal gene copy number:   0%|          | 0/3668 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "samples\n",
       "VBS50275-6645STDY11194051    3\n",
       "VBS50276-6645STDY11194052    2\n",
       "VBS50283-6645STDY11194053    0\n",
       "VBS50286-6645STDY11194055    2\n",
       "VBS50287-6645STDY11194056    4\n",
       "                            ..\n",
       "VBS67021-6645STDY13057665    2\n",
       "VBS67022-6645STDY13057666    1\n",
       "VBS67024-6645STDY13057668    2\n",
       "VBS67025-6645STDY13057669    2\n",
       "VBS67026-6645STDY13057670    3\n",
       "Length: 1005, dtype: int8"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Identify tagging snps for duplicated genes\n",
    "\n",
    "# Identify samples that have main dups # do this for all dup variants of that genes that you are interested in\n",
    "discordant_read_calls_dup1a = (\n",
    "    ag3\n",
    "    .cnv_discordant_read_calls(contig=\"2R\", \n",
    "                               sample_sets=sample_sets,\n",
    "                               sample_query = sample_query\n",
    "                               )\n",
    "    .set_index(samples=\"sample_id\", variants=\"variant_id\")\n",
    "    .sel(variants='Cyp6aap_Dup1a') #if you don't know the CNV variants, check by running this code without including\n",
    "                                   #.sel then view by running discordant_read_calls.variants.value\n",
    "    .sel(samples=meta.index.astype(str))\n",
    ")\n",
    "#discordant_read_calls_dup1a\n",
    "Dup1_calls = discordant_read_calls_dup1a.call_genotype.values\n",
    "\n",
    "\n",
    "\n",
    "# get a filter for excluding samples with poor quality HMM data\n",
    "gene_copy_number = (\n",
    "    ag3.gene_cnv(region=\"2R\", \n",
    "                 sample_sets=sample_sets,\n",
    "                 sample_query = sample_query,\n",
    "                 max_coverage_variance = None\n",
    ")\n",
    "    .set_index(genes=\"gene_id\", samples=\"sample_id\")\n",
    "    .sel(samples=meta.index.astype(str))\n",
    ")\n",
    "\n",
    "# Select data for genes of interest, excluding samples with poor quality HMM data\n",
    "cyp6aa1_gene_copy_number = (\n",
    "    gene_copy_number[\"CN_mode\"]\n",
    "    .sel(genes='AGAP002862')\n",
    "    .transpose()\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "good_variance = ~gene_copy_number.sample_is_high_variance.values\n",
    "\n",
    "\n",
    "Dup1_copy_number = (cyp6aa1_gene_copy_number[good_variance]-2) * Dup1_calls[good_variance]\n",
    "meta['dup1_copy_number'] = Dup1_copy_number.reindex(meta.index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MannwhitneyuResult(statistic=55077.5, pvalue=0.3521640721682472)\n"
     ]
    }
   ],
   "source": [
    "#H3: dup1 copy number  change between round 1 and 5\n",
    "meta=meta.dropna(subset=['dup1_copy_number'])\n",
    "# Separate the dup1_copy_number values for each round\n",
    "dup1_round1 = meta[meta['RND'] == 1]['dup1_copy_number']\n",
    "dup1_round5 = meta[meta['RND'] == 5]['dup1_copy_number']\n",
    "\n",
    "# Perform the Mann-Whitney U test\n",
    "result = stats.mannwhitneyu(dup1_round1, dup1_round5)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MannwhitneyuResult(statistic=24634.0, pvalue=0.14629999825598808)\n"
     ]
    }
   ],
   "source": [
    "#H4 dup1 copy number  change between round 1 and 5 in nonpbo\n",
    "meta_npbo= meta[(meta['llin_actual']=='Non-PBO LLIN')]\n",
    "meta_npbo=meta_npbo.dropna(subset=['dup1_copy_number'])\n",
    "# Separate the dup1_copy_number values for each round\n",
    "dup1_round1 = meta_npbo[meta_npbo['RND'] == 1]['dup1_copy_number']\n",
    "dup1_round5 = meta_npbo[meta_npbo['RND'] == 5]['dup1_copy_number']\n",
    "\n",
    "# Perform the Mann-Whitney U test\n",
    "result = stats.mannwhitneyu(dup1_round1, dup1_round5)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MannwhitneyuResult(statistic=4377.0, pvalue=0.370947240300443)\n"
     ]
    }
   ],
   "source": [
    "#h4 dup1 copy number  change between round 1 and 5 in pbo\n",
    "meta_pbo= meta[(meta['llin_actual']=='PBO LLIN')]\n",
    "meta_pbo=meta_pbo.dropna(subset=['dup1_copy_number'])\n",
    "# Separate the dup1_copy_number values for each round\n",
    "dup1_round1 = meta_pbo[meta_pbo['RND'] == 1]['dup1_copy_number']\n",
    "dup1_round5 = meta_pbo[meta_pbo['RND'] == 5]['dup1_copy_number']\n",
    "\n",
    "# Perform the Mann-Whitney U test\n",
    "result = stats.mannwhitneyu(dup1_round1, dup1_round5)\n",
    "\n",
    "# Print the result\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     \r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7aaa830e60e0454581e85d96fc928188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Load CNV HMM data:   0%|          | 0/176 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e97a268b8c204f63ae0ca15f3d6a58fb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Compute modal gene copy number:   0%|          | 0/3668 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1328677/2610785262.py:64: ConstantInputWarning: An input array is constant; the correlation coefficient is not defined.\n",
      "  SNP_cor_dup1 = np.apply_along_axis(lambda x: spearmanr((x>0).astype('int'),\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Dup1 proxy SNP is at positions 28470419\n",
      "The maximum correlation coefficient with the Dup is 0.5732864158771411\n"
     ]
    }
   ],
   "source": [
    "# Identify tagging snps for duplicated genes\n",
    "\n",
    "# Identify samples that have main dups # do this for all dup variants of that genes that you are interested in\n",
    "discordant_read_calls_dup1a = (\n",
    "    ag3\n",
    "    .cnv_discordant_read_calls(contig=\"2R\", \n",
    "                               sample_sets=sample_sets,\n",
    "                               sample_query = sample_query\n",
    "                               )\n",
    "    .set_index(samples=\"sample_id\", variants=\"variant_id\")\n",
    "    .sel(variants='Cyp6aap_Dup1a') #if you don't know the CNV variants, check by running this code without including\n",
    "                                   #.sel then view by running discordant_read_calls.variants.value\n",
    "    .sel(samples=meta.index.astype(str))\n",
    ")\n",
    "#discordant_read_calls_dup1a\n",
    "Dup1_calls = discordant_read_calls_dup1a.call_genotype.values\n",
    "\n",
    "\n",
    "\n",
    "# get a filter for excluding samples with poor quality HMM data\n",
    "gene_copy_number = (\n",
    "    ag3.gene_cnv(region=\"2R\", \n",
    "                 sample_sets=sample_sets,\n",
    "                 sample_query = sample_query,\n",
    "                 max_coverage_variance = None\n",
    ")\n",
    "    .set_index(genes=\"gene_id\", samples=\"sample_id\")\n",
    "    .sel(samples=meta.index.astype(str))\n",
    ")\n",
    "\n",
    "# Select data for genes of interest, excluding samples with poor quality HMM data\n",
    "cyp6aa1_gene_copy_number = (\n",
    "    gene_copy_number[\"CN_mode\"]\n",
    "    .sel(genes='AGAP002862')\n",
    "    .transpose()\n",
    "    .to_pandas()\n",
    ")\n",
    "\n",
    "good_variance = ~gene_copy_number.sample_is_high_variance.values\n",
    "\n",
    "\n",
    "Dup1_copy_number = (cyp6aa1_gene_copy_number[good_variance]-2) * Dup1_calls[good_variance]\n",
    "\n",
    "#now we get the snp tag for dup1\n",
    "\n",
    "#haplotype calls withing the region where the dup is\n",
    "region='2R:28470000-28500000'\n",
    "\n",
    "haps = (\n",
    "    ag3.haplotypes(region = region, \n",
    "                   analysis = 'gamb_colu', \n",
    "                   sample_sets = sample_sets,\n",
    "                   sample_query = sample_query\n",
    "                   )\n",
    "    .set_index(samples = 'sample_id')\n",
    "    .sel(samples=meta.index.astype(str))\n",
    ")\n",
    "\n",
    "haps_Dup = haps.isel(samples = good_variance)\n",
    "\n",
    "# Create haplotype array\n",
    "hap_array_Dup = allel.GenotypeArray(haps_Dup.call_genotype).to_haplotypes()\n",
    "\n",
    "# Get allele counts\n",
    "ac = hap_array_Dup.count_alleles()\n",
    "\n",
    "# Identify segregating non-singletons. \n",
    "non_singleton = ac.min(1) > 1\n",
    "\n",
    "# Filter the datasets, removing non-segregating and singleton variants \n",
    "haps_Dup = haps_Dup.isel(variants = np.where(non_singleton)[0])\n",
    "genotypes = haps_Dup.call_genotype.values.sum(2)\n",
    "\n",
    "\n",
    "SNP_cor_dup1 = np.apply_along_axis(lambda x: spearmanr((x>0).astype('int'), \n",
    "                                                  Dup1_calls[good_variance]), \n",
    "                                    1, \n",
    "                                    genotypes)[:, 0]\n",
    "SNP_cor_dup1 = np.abs(SNP_cor_dup1)\n",
    "\n",
    "Dup1_proxy_SNP_position = haps_Dup.variant_position[np.nanargmax(SNP_cor_dup1)].values\n",
    "Dup1_proxy = genotypes[np.nanargmax(SNP_cor_dup1)]\n",
    "\n",
    "\n",
    "print(f'The Dup1 proxy SNP is at positions {Dup1_proxy_SNP_position}')\n",
    "\n",
    "print(f'The maximum correlation coefficient with the Dup is {np.nanmax(SNP_cor_dup1)}')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  \r"
     ]
    }
   ],
   "source": [
    "\n",
    "#Specify the colours and labels  of information  to add on the dendongram\n",
    "\n",
    "\n",
    "#Location and other rows you want to visualise\n",
    "location = np.unique(meta.Location)\n",
    "num_location = len(location)\n",
    "location_palette = sns.color_palette('viridis', num_location)\n",
    "location_colours = {location[i]: location_palette[i] for i in range(num_location)}\n",
    "\n",
    "#control_phase_colours = {'pre': 'olive', 'post': 'khaki', 'intermediate':'#BDB76B'}\n",
    "\n",
    "\n",
    "\n",
    "# Now add extra rows for the SNPs and dup proxy\n",
    "\n",
    "#dup tagging snps\n",
    "dup1_proxy =28470419\n",
    "dup5_proxy =28471934\n",
    "\n",
    "#cyp6p region snps \n",
    "\n",
    "region = '2R:28463444-28499726'\n",
    "array_of_all_snps =np.array([28470419,28471934,28492268]) # include all snps you want to plot and also snps tagging the dups\n",
    "array_of_all_snps_labels = ['Cyp6aa1_dup1', 'Cyp6aa1_dup','Cyp6p3_L292V'] # name of the snps if you want to specify, if not include the snps pos as above but with ''\n",
    "\n",
    "\n",
    "\n",
    "#plot the haplotypes in the region of interest as determined by H12 peak modelling\n",
    "\n",
    "haps = (\n",
    "    ag3.haplotypes(region = region, \n",
    "                   analysis = 'gamb_colu', \n",
    "                   sample_sets = sample_sets,\n",
    "                   sample_query = sample_query)\n",
    "    .set_index(samples = 'sample_id')\n",
    "    .sel(samples = meta.index.astype(str))\n",
    "\n",
    ")\n",
    "hapsamples = np.array(np.repeat(haps['samples'], 2))\n",
    "\n",
    "location_labels = meta.loc[hapsamples, 'Location'].values\n",
    "#control_phase_labels = meta.set_index('sample_id').loc[hapsamples, 'control_phase'].values\n",
    "\n",
    "\n",
    "haparray = allel.GenotypeArray(haps['call_genotype']).to_haplotypes()\n",
    "hapnames = np.concatenate([[x + 'a', x + 'b'] for x in haps['samples'].values])\n",
    "\n",
    "dist = allel.pairwise_distance(haparray, metric = 'hamming')\n",
    "\n",
    "site_filter = ag3.snp_calls(region=region, sample_sets=sample_sets)['variant_filter_pass_gamb_colu']\n",
    "n_bases = np.sum(site_filter.values)\n",
    "dist_dxy = dist * haparray.n_variants / n_bases\n",
    "\n",
    "\n",
    "min_cluster_size = 20\n",
    "# A function to identify haplotyde clusters from a distance matrix.\n",
    "def find_clusters(dist, n, threshold=0.001, method='complete'):\n",
    "        # build hierarchy\n",
    "        clust = scipy.cluster.hierarchy.linkage(dist, method=method)\n",
    "        # find clusters\n",
    "        f = scipy.cluster.hierarchy.fcluster(clust, threshold,\n",
    "                                             criterion='distance')\n",
    "        # compute cluster sizes\n",
    "        fsz = np.bincount(f)\n",
    "        # sort largest first\n",
    "        fsort = np.argsort(fsz)[::-1]\n",
    "        # take largest n\n",
    "        fsort = fsort[:n]\n",
    "        # get haplotype indices for each cluster\n",
    "        clusters = [set(np.nonzero(f == i)[0]) for i in fsort]\n",
    "        return clusters\n",
    "\n",
    "focal_clusters = find_clusters(dist_dxy, n=20, threshold=0.001)\n",
    "large_focal_clusters = [cluster for cluster in focal_clusters if len(cluster) > min_cluster_size]\n",
    "focal_cluster1_haps = hapnames[list(large_focal_clusters[0])]\n",
    "\n",
    "\n",
    "# Function to assign sample haplotypes to clusters\n",
    "def assign_sample_haplotypes_parallel(cluster_ids):\n",
    "    cluster_samples = np.array(hapsamples[list(cluster_ids)]).flatten()\n",
    "    sample_ids = meta.index.astype(str).values\n",
    "    sample_hap_genotype = np.array([np.sum(np.isin(cluster_samples, sample)) for sample in sample_ids])\n",
    "    return sample_hap_genotype\n",
    "\n",
    "# Define number of processes\n",
    "num_processes = min(20, multiprocessing.cpu_count())  # Ensures it doesn’t exceed available cores\n",
    "\n",
    "# Run in parallel\n",
    "with multiprocessing.Pool(processes=num_processes) as pool:\n",
    "    cluster_assignments = pool.map(assign_sample_haplotypes_parallel, large_focal_clusters)\n",
    "\n",
    "# Store results in meta DataFrame\n",
    "for i, assignment in enumerate(cluster_assignments):\n",
    "    meta[f'cluster_{i+1}'] = assignment\n",
    "\n",
    "# Prepare the output table\n",
    "output_columns = ['Location', 'llin_actual', 'control_phase', 'RND', 'HSD'] + [f'cluster_{i+1}' for i in range(len(large_focal_clusters))]\n",
    "output_table = meta[output_columns]\n",
    "\n",
    "# Save to CSV\n",
    "output_filename = f'cluster_assignments_{region}.csv'\n",
    "output_table.to_csv(output_filename, sep='\\t', index_label='sample_name')\n",
    "\n",
    "\n",
    "z1 = scipy.cluster.hierarchy.linkage(dist_dxy, method=\"complete\")\n",
    "r1 = scipy.cluster.hierarchy.dendrogram(\n",
    "        z1, no_labels=True, count_sort=True,\n",
    "        color_threshold=0, no_plot=True,\n",
    "        above_threshold_color='k')\n",
    "\n",
    "# Function to find the span of each cluster\n",
    "\n",
    "def truspan(cluster, r):\n",
    "\t# get the index of the cluster haps in the dendrogram list of all haps\n",
    "\tcluster_leaves = sorted([r['leaves'].index(i) for i in cluster])\n",
    "\t# are these indices monotonic - they should be!\n",
    "\tx = np.asarray(cluster_leaves)\n",
    "\tdx = np.diff(x)\n",
    "\tmon = np.all(dx == 1)\n",
    "\tassert mon\n",
    "\treturn min(cluster_leaves), max(cluster_leaves)\n",
    "\n",
    "v_span = [truspan(cluster, r1) for cluster in large_focal_clusters]\n",
    "cluster_lab = ['C' + str(i+1) for i in range(len(large_focal_clusters))]\n",
    "\n",
    "\n",
    "# add snps to the plot\n",
    "\n",
    " #full_region = f'2R:{np.min(array_of_all_snps)}-{np.max(array_of_all_snps)}' #change chrom\n",
    "full_region = '2L:2300000-2900000'\n",
    "\n",
    "full_region_haps = (ag3.haplotypes(\n",
    "    region = region,\n",
    "    analysis = 'gamb_colu',\n",
    "    sample_sets = sample_sets,\n",
    "    sample_query = sample_query,)\n",
    "    .set_index(samples = 'sample_id')\n",
    "    .sel(samples = meta.index.astype(str)))\n",
    "\n",
    "indexed_haps = full_region_haps.set_index(variants = 'variant_position')\n",
    "snp_calls = allel.GenotypeArray(indexed_haps.sel(variants = array_of_all_snps).call_genotype).to_haplotypes()\n",
    "#colour schemes\n",
    "snp_colour_scheme = {0: 'lightgrey', 1: 'black'}\n",
    "\n",
    "dup1_colour_scheme = {0: 'lightgrey', 1: 'blue'}\n",
    "dup5_colour_scheme = {0: 'lightgrey', 1: 'red'}\n",
    "\n",
    "#these_labels = [location_labels, control_phase_labels]\n",
    "these_labels = [location_labels]#, kdr_haps_labels]\n",
    "#these_colour_schemes = [location_colours, control_phase_colours]\n",
    "these_colour_schemes = [location_colours]#, kdr_haps_colours]\n",
    "#these_label_names = ['location', 'control phase']\n",
    "these_label_names = ['location']#, 'KDR haps']\n",
    "for snp, snp_label in zip(array_of_all_snps, array_of_all_snps_labels):\n",
    "     #if no dups then comment out\n",
    "    if snp == dup1_proxy:\n",
    "        these_labels.append(snp_calls[list(array_of_all_snps).index(snp)])\n",
    "        these_colour_schemes.append(dup1_colour_scheme)  # Apply different color scheme \n",
    "        these_label_names.append('Cyp6aa1_dup1')  # Custom label instead of SNP position\n",
    "          #if no dups then comment out\n",
    "    elif snp == dup5_proxy:\n",
    "        these_labels.append(snp_calls[list(array_of_all_snps).index(snp)])\n",
    "        these_colour_schemes.append(dup5_colour_scheme)  # Apply different color scheme\n",
    "        these_label_names.append('Cyp6aa1_dup5')  # Custom label instead of SNP position\n",
    "          #if no snps then comment out\n",
    "    else:\n",
    "        these_labels.append(snp_calls[list(array_of_all_snps).index(snp)])\n",
    "        these_colour_schemes.append(snp_colour_scheme)\n",
    "        these_label_names.append(snp_label)\n",
    "        #these_label_names.append(array_of_all_snps_labels)   # Keep original SNP labels\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def draw_hap_cluster_plot2(z, h, \n",
    "                          colour_schemes, labels, label_names,\n",
    "                          cluster_labels, vspans,\n",
    "                          add_legend=True, title='',\n",
    "                          fn=None):\n",
    "    # Adjust GridSpec to include the cluster brackets\n",
    "    gs = GridSpec(len(colour_schemes) + 2, 1, height_ratios=[6.0] + [0.4] * len(colour_schemes) + [0.8])\n",
    "    fig = plt.figure(figsize=(15, (7 + 0.58 * len(colour_schemes) + 1)))\n",
    "    plt.subplots_adjust(hspace=0.02)\n",
    "    ax1 = plt.subplot(gs[0])\n",
    "    sns.despine(ax=ax1, offset=5, bottom=True, top=True)\n",
    "    r = scipy.cluster.hierarchy.dendrogram(z, no_labels=True, count_sort=True,\n",
    "                                           color_threshold=0,\n",
    "                                           above_threshold_color='k',\n",
    "                                           ax=ax1)\n",
    "    ax1.spines['top'].set_visible(False)\n",
    "    ax1.set_xticks([])\n",
    "    ax1.set_ylim(0)\n",
    "    \n",
    "    for i in range(len(colour_schemes)):\n",
    "        this_ax = fig.add_subplot(gs[i + 1])\n",
    "        l = np.array([labels[i][hap] for hap in r['leaves']])\n",
    "        \n",
    "        # Color SNP haplotypes dynamically\n",
    "        hap_clrs = [colour_schemes[i].get(p, 'lightgrey') for p in l]\n",
    "        \n",
    "        this_ax.broken_barh(xranges=[(j, 1) for j in range(h.shape[1])],\n",
    "                            yrange=(0, 1),\n",
    "                            color=hap_clrs)\n",
    "        sns.despine(ax=this_ax, offset=5, left=True, bottom=True, top=True)\n",
    "        this_ax.set_xticks([])\n",
    "        this_ax.set_yticks([])\n",
    "        this_ax.set_xlim(0, h.shape[1])\n",
    "        this_ax.yaxis.set_label_position('left')\n",
    "        this_ax.set_ylabel(label_names[i], rotation=0, ha='right', va='center')\n",
    "    \n",
    "    # Add a cleaner legend dynamically\n",
    "    if add_legend:\n",
    "        legend_patches = []\n",
    "\n",
    "        # Add categorical variables first\n",
    "        for val in np.unique(labels[0]):  # Location\n",
    "            if val in colour_schemes[0]:\n",
    "                legend_patches.append(mpatches.Patch(color=colour_schemes[0][val], label=f'Location: {val}'))\n",
    "        \n",
    "        # Add SNPs as a single category\n",
    "        legend_patches.append(mpatches.Patch(color='black', label='SNP: Alt'))\n",
    "        legend_patches.append(mpatches.Patch(color='lightgrey', label='SNP: Ref'))\n",
    "        \n",
    "        # Explicitly add `dup1` and `dup5`  # comment out if no dups\n",
    "        legend_patches.append(mpatches.Patch(color='blue', label='Dup1'))\n",
    "        legend_patches.append(mpatches.Patch(color='red', label='Dup5'))\n",
    "        legend_patches.append(mpatches.Patch(color='lightgrey', label='No_dup'))\n",
    "        ax1.legend(handles=legend_patches, title=\"\", loc='right', frameon=False)\n",
    "    \n",
    "    # Cluster brackets subplot\n",
    "    ax_clu = fig.add_subplot(gs[-1])\n",
    "    sns.despine(ax=ax_clu, bottom=True, left=True)\n",
    "    ax_clu.set_xlim(0, h.shape[1])\n",
    "    ax_clu.set_ylim(0, 1)\n",
    "    \n",
    "    for lbl, (xmin, xmax) in zip(cluster_labels, vspans):\n",
    "        if lbl:\n",
    "            fraction = -20 / (xmax - xmin)\n",
    "            ax_clu.annotate(\"\", ha='left', va='center',\n",
    "                            xy=(xmin, 1), xycoords='data',\n",
    "                            xytext=(xmax, 1), textcoords='data',\n",
    "                            arrowprops=dict(arrowstyle=\"-\",\n",
    "                                            connectionstyle=\"bar,fraction=%.4f\" % fraction))\n",
    "            ax_clu.text((xmax + xmin) / 2, 0.1, lbl, va='top', ha='center')\n",
    "    \n",
    "    ax_clu.set_xticks([])\n",
    "    ax_clu.set_yticks([])\n",
    "    ax1.set_title(f'{title}')\n",
    "    \n",
    "    fig.patch.set_alpha(1)\n",
    "    \n",
    "    if fn:\n",
    "        ext = re.sub('.*\\\\.', '', fn)\n",
    "        plt.savefig(fn, format=ext)\n",
    "    \n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot region 1 haplotypes while viewing the SNPs of region2\n",
    "draw_hap_cluster_plot2(z1, haparray, labels=these_labels,\n",
    "                       cluster_labels=cluster_lab,\n",
    "                      vspans=v_span,\n",
    "                      colour_schemes = these_colour_schemes,\n",
    "                      label_names = these_label_names,\n",
    "\t\t\t\t\t  fn = 'haplotype_cluster_test.png'\n",
    ")\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gaard",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
